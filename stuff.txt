import json
import subprocess

import pandas as pd
import re

df = pd.read_excel('bridging_video_list_20240424.xlsx')
df = df[['video_id', 'camera_name', 'Bridging_in_video', 'Bridged_mouth', 'bridge_start',
         'bridge_end', 'dirty_lens', 'split']]


# Define a function to extract datetime from video_id
def extract_datetime(video_id):
    match = re.search(r'\d{4}-\d{2}-\d{2}-\d{2}-\d{2}-\d{2}', video_id)
    if match:
        return match.group(0)
    match = re.search(r'\d{4}-\d{2}-\d{2}_\d{2}-\d{2}-\d{2}', video_id)
    if match:
        return match.group(0).replace('_', '-')
    return None


# Splits time to convert to object for ime addition
def convert_to_timedelta(time_str):
    try:
        parts = time_str.split(':')
        hours = int(parts[0])
        minutes = int(parts[1])
        seconds = int(parts[2])
        return pd.Timedelta(hours=hours, minutes=minutes, seconds=seconds)
    except Exception as e:
        return pd.NaT


df['datetime'] = df['video_id'].apply(extract_datetime)
df['datetime'] = pd.to_datetime(df['datetime'], format='%Y-%m-%d-%H-%M-%S')
df = df.sort_values(by='datetime')


def process_row(data):
    data = data[data['bridge_start'].notnull()]
    df = data.copy()
    df['ts_start'] = df['datetime'] + df['bridge_start'].astype(str).apply(convert_to_timedelta)
    df['ts_finish'] = df['datetime'] + df['bridge_end'].astype(str).apply(convert_to_timedelta)
    df = df[df['ts_start'].notnull()]

    df['ts_start'] = df['ts_start'].dt.strftime('%Y-%m-%dT%H:%M:%S') + '+08:00'
    df['ts_finish'] = df['ts_finish'].dt.strftime('%Y-%m-%dT%H:%M:%S') + '+08:00'

    return df[['camera_name', 'datetime', 'ts_start', 'ts_finish', 'Bridging_in_video']]


def generate_json(data):
    json_dic = []

    for index, row in data.iterrows():
        jsons = {
            'camera_name': row['camera_name'],
            'ts_start': row['ts_start'],
            'ts_finish': row['ts_finish'],
            'labels': True if row['Bridging_in_video'] else False
        }

        json_dic.append(jsons)

    return json_dic


processed_data = process_row(df)
json_list = generate_json(processed_data)

for json_data in json_list:
    json_string = json.dumps(json_data)

    subprocess.run(['aws', 'lambda', 'invoke', '--function-name',
                    'dev-IroccvDMSStack-VideoLabelsUpsertFunction3E924D-AK6ainIn36uI', '--invocation-type',
                    'Event', '--payload', f'file:///dev/stdin', '--cli-binary-format', 'raw-in-base64-out'],
                   input=json_string.encode('utf-8'))


























#!/bin/bash
for filename in *.json; do
    echo "Video $filename starting"
    aws lambda invoke --function-name "dev-IroccvDMSStack-VideoLabelsUpsertFunction3E924D-AK6ainIn36uI" --invocation-type Event --payload file:"//$filename" --cli-binary-format raw-in-base64-out /dev/stdout
    echo "Video $filename complete"
done


import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt


def plot_predicted_vs_observed(df, test_column_prefix='model_test_'):
    observed = df['bridge_start'].astype('timedelta64[s]').values / np.timedelta64(1, 's')  # Convert to seconds
    predicted_columns = [col for col in df.columns if col.startswith(test_column_prefix)]

    rmse_values = []

    plt.figure(figsize=(10, 6))

    for col in predicted_columns:
        predicted = df[col].apply(lambda x: pd.Timedelta(x).total_seconds()).values

        if not np.all(predicted == 0):  # Check if all values are zero
            predicted = np.where(predicted > 0, predicted, np.nan)  # Convert non-positive predictions to NaN
            not_nan_indices = ~np.isnan(predicted)
            predicted = predicted[not_nan_indices]
            observed_subset = observed[not_nan_indices]

            lr = LinearRegression()
            lr.fit(observed_subset.reshape(-1, 1), predicted)

            rmse = np.sqrt(mean_squared_error(observed_subset, lr.predict(observed_subset.reshape(-1, 1))))
            rmse_values.append(rmse)

            plt.scatter(observed_subset, predicted, label=col)
            plt.plot(observed_subset, lr.predict(observed_subset.reshape(-1, 1)), label=f'Regression Line ({col})')

    plt.scatter(observed, observed, label='Observed', color='red')

    # Plotting settings
    plt.xlabel('Observed (bridge_start)')
    plt.ylabel('Predicted')
    plt.legend()
    plt.title('Observed vs Predicted')
    plt.tight_layout()
    plt.show()

    return rmse_values


def calculate_rmse_and_variance(df, test_column_prefix='model_test_'):
    predicted_columns = [col for col in df.columns if col.startswith(test_column_prefix)]
    rmse_values = []

    # Loop through each observation
    for index, row in df.iterrows():
        predicted_values = []

        # Calculate RMSE for each predicted column
        for col in predicted_columns:
            predicted = pd.Timedelta(row[col]).total_seconds()
            if predicted > 0:  # Check if the prediction is positive
                predicted_values.append(predicted)

        if predicted_values:
            observed_time = pd.Timedelta(row['bridge_start']).total_seconds()

            # RMSE for this observation
            rmse = np.sqrt(np.mean([(observed_time - predicted_time) ** 2 for predicted_time in predicted_values]))
            rmse_values.append(rmse)

    return rmse_values


df = pd.read_csv('bridge_predicted_observed.csv')
rmse_all = plot_predicted_vs_observed(df)
print(rmse_all)
rmses = calculate_rmse_and_variance(df)
for i, (rmse) in enumerate(rmses, start=1):
    print(f"Observation {i}: RMSE = {rmse}")





























buffer = BytesIO()
plt.savefig(buffer, format='png')
buffer.seek(0)

# Upload the plot to SharePoint
upload_url = 'YOUR_SHAREPOINT_UPLOAD_URL_HERE'
files = {'file': ('frame_distribution_plot.png', buffer, 'image/png')}
response = requests.post(upload_url, files=files)

if response.status_code == 200:
    print("Plot uploaded successfully to SharePoint.")
else:
    print("Error uploading plot to SharePoint.")


import datetime

import pandas as pd
import matplotlib.pyplot as plt
import re

df = pd.read_excel('bridging_video_list_20240424.xlsx')
df = df[(df['Bridging_in_video'] == True) & (df['split'] == 'val')]

test_columns = ['bridge_start', 'model_test_1_2024-05-12-10-30-00',
                'model_test_2_2024-05-13-11-30-00']  # Add more columns as needed


# Define a function to extract datetime from video_id
def extract_datetime(video_id):
    match = re.search(r'\d{4}-\d{2}-\d{2}-\d{2}-\d{2}-\d{2}', video_id)
    if match:
        return match.group(0)
    match = re.search(r'\d{4}-\d{2}-\d{2}_\d{2}-\d{2}-\d{2}', video_id)
    if match:
        return match.group(0).replace('_', '-')
    return None


# Splits time to convert to object for ime addition
def convert_to_timedelta(time_str):
    try:
        parts = time_str.split(':')
        hours = int(parts[0])
        minutes = int(parts[1])
        seconds = int(parts[2])
        return pd.Timedelta(hours=hours, minutes=minutes, seconds=seconds)
    except Exception as e:
        # print(f"Error converting {time_str} to timedelta: {e}")
        return pd.NaT


df['datetime'] = df['video_id'].apply(extract_datetime)
df['datetime'] = pd.to_datetime(df['datetime'], format='%Y-%m-%d-%H-%M-%S')
df = df.sort_values(by='datetime')

test_name = None

plt.figure(figsize=(15, 8))

for i, test_col in enumerate(test_columns, start=1):
    df[f'bridge_event_time_{i}'] = df[test_col].astype(str).apply(convert_to_timedelta)
    df[f'bridge_event_time_{i}'] = df['datetime'] + df[f'bridge_event_time_{i}']

    if test_col == 'bridge_start':
        test_name = 'Baseline'
    else:
        test_name = test_col

    plt.plot(df[f'bridge_event_time_{i}'], [i] * len(df), marker='o', linestyle='-', label=f'{test_name}')

plt.xlabel('Timestamp')
plt.ylabel('Video ID Index')
plt.title('Bridge Events')
plt.grid(True)

plt.xticks(rotation=0)
plt.ylim(0, 5)
plt.tight_layout()
plt.legend()


# plt.show()


def calculate_time_delta(df, test_columns):
    for k, test_col in enumerate(test_columns, start=1):
        df[f'bridge_event_time_{k}'] = df[test_col].astype(str).apply(convert_to_timedelta)
        df[f'bridge_event_time_{k}'] = df['datetime'] + df[f'bridge_event_time_{k}']

        if test_col != 'bridge_start':
            delta_col_name = f'{test_col}'
            df[test_col] = pd.to_datetime(df[test_col], format='%H:%M:%S')
            df[delta_col_name] = df[test_col] - pd.to_datetime(df['bridge_start'], format='%H:%M:%S')

    model_test_columns = [col for col in df.columns if 'model_test' in col]

    for column in model_test_columns:
        df[column] = df[column].astype(str).map(lambda x: x[7:] if x != '0 days' else '0')

    return df

# Example usage
df_result = calculate_time_delta(df, test_columns)
print(df[test_columns].to_string())









import pandas as pd
from office365.runtime.auth.user_credential import UserCredential
from office365.sharepoint.client_context import ClientContext
from office365.sharepoint.files.file import File
from io import BytesIO
import requests

# Upload the file to SharePoint
def upload_to_sharepoint(local_file_path):
    user_credentials = UserCredential(user, passwrd)
    ctx = ClientContext(sharepoint_url).with_credentials(user_credentials)
    with open(local_file_path, 'rb') as file:
        content = BytesIO(file.read())
    response = File.save_binary(ctx, f"{sharepoint_folder}{local_file_path.split('/')[-1]}", content)
    if response.status_code == 200:
        print("File uploaded successfully to SharePoint.")
    else:
        print(f"Error uploading file to SharePoint: {response.text}")

upload_to_sharepoint(local_file_path)

print(f'File downloaded and saved to: {file_path}')

 




import concurrent.futures
import boto3

# Assuming you have already initialized boto3 and dy_table_name

def fetch_items(video):
    items = []
    last_evaluated_key = None
    
    while True:
        # Query DynamoDB with LastEvaluatedKey if available
        if last_evaluated_key:
            response = table.query(
                KeyConditionExpression=Key('video_id').eq(video),
                ExclusiveStartKey=last_evaluated_key
            )
        else:
            response = table.query(
                KeyConditionExpression=Key('video_id').eq(video)
            )
        
        # Extend items with fetched items
        items.extend(response['Items'])
        
        # Update LastEvaluatedKey if there are more pages
        last_evaluated_key = response.get('LastEvaluatedKey')
        
        # Break the loop if there are no more pages
        if not last_evaluated_key:
            break
            
    return items

# Number of threads to use for parallel processing
num_threads = 10  # Adjust this as per your requirements

# List of videos to fetch items for
videos_to_fetch = matching_ids_list

items = []

# Create a ThreadPoolExecutor
with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
    # Submit fetch_items function for each video
    futures = [executor.submit(fetch_items, video) for video in videos_to_fetch]

    # Iterate through completed futures
    for future in concurrent.futures.as_completed(futures):
        try:
            # Get the result of the future
            items.extend(future.result())
        except Exception as e:
            # Handle any exceptions raised during execution
            print(f"Exception occurred: {e}")

# Now 'items' list contains items fetched in parallel










data['min_group'] = data['frame_ts'].dt.hour * 60 + data['frame_ts'].dt.minute
frame_counts = data.groupby('min_group').size().reset_index(name='frame_count')

# Create a range of 10-minute intervals covering the entire 24-hour period
min_range = range(0, 24 * 60, 30)  # Adjust the step size to 30 minutes

# Plot the distribution
plt.figure(figsize=(15, 8))
sns.histplot(data=frame_counts, x='min_group', bins=min_range, kde=True)
plt.xlabel('Frame Timestamp (Hour:Minute)')
plt.ylabel('Frequency')
plt.title('Distribution of Frame Timestamps for Each 24-Hour Period')

# Add markers for bridged events
bridged_data = data[data['bridged'] == True]
plt.scatter(bridged_data['min_group'], [0]*len(bridged_data), marker='_',   color='red', label='Bridged Events')

plt.xticks(min_range, [f'{hour:02d}:{minute:02d}' for hour in range(24) for minute in range(0, 60, 30)], rotation=45)
plt.tight_layout()
plt.legend()
plt.savefig('frame_distribution_with_bridged_events.png')
plt.show()


def get_matching_frames(id_list):
    # for some reason, need to construct uri, unpick and parse
    prefix_frames = []
    
    for i in id_list:
        uri = f's3://{bucket_dms}/{frames_dms}{i}/'
        parsed_uri = urlparse(uri)

        # extract bucket name and object key prefix from the parsed URI
        bucket_name = parsed_uri.netloc
        prefix = parsed_uri.path.lstrip('/')
        
        # paginate through the results to retrieve all objects
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

        for page in pages:
            object_keys = [obj['Key'] for obj in page.get('Contents', [])]
            prefix_frames.extend(object_keys)

    return prefix_frames

prefix_list = get_matching_frames(list(matching_ids_list))


import concurrent.futures
import boto3
from urllib.parse import urlparse

# Initialize S3 client
s3 = boto3.client('s3')

def get_matching_frames(id_list):
    prefix_frames = []

    def fetch_prefix_frames(uri):
        parsed_uri = urlparse(uri)
        bucket_name = parsed_uri.netloc
        prefix = parsed_uri.path.lstrip('/')

        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

        object_keys = []
        for page in pages:
            object_keys.extend([obj['Key'] for obj in page.get('Contents', [])])

        return object_keys

    # Use ThreadPoolExecutor to run fetch_prefix_frames asynchronously
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_to_uri = {executor.submit(fetch_prefix_frames, f's3://{bucket_dms}/{frames_dms}{i}/'): i for i in id_list}
        for future in concurrent.futures.as_completed(future_to_uri):
            uri = future_to_uri[future]
            try:
                object_keys = future.result()
                prefix_frames.extend(object_keys)
            except Exception as exc:
                print(f'Error fetching frames for {uri}: {exc}')

    return prefix_frames

prefix_list = get_matching_frames(matching_ids_list)











def get_matching_frames(id_list):
    # for some reason, need to construct uri, unpick and parse
    prefix_frames = []
    
    for i in id_list:
        uri = f's3://{bucket_dms}/{frames_dms}{i}/'
        parsed_uri = urlparse(uri)

        # extract bucket name and object key prefix from the parsed URI
        bucket_name = parsed_uri.netloc
        prefix = parsed_uri.path.lstrip('/')
        
        # paginate through the results to retrieve all objects
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

        for page in pages:
            object_keys = [obj['Key'] for obj in page.get('Contents', [])]
            prefix_frames.extend(object_keys)

    return prefix_frames

prefix_list = get_matching_frames(list(matching_ids_list))
print(len(prefix_list))





















import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv('df2.csv', usecols=['frame_ts'])
data['frame_ts'] = pd.to_datetime(data['frame_ts'], unit='ms')

grouped_data = data.groupby(data['frame_ts'].dt.floor('D'))

fig, ax = plt.subplots(figsize=(15, 8))

# Iterate over each day and plot the distribution
for day, day_data in grouped_data:
    # Group data into 10-minute intervals within the current day and count the number of frames
    day_data['min_group'] = day_data['frame_ts'].dt.hour * 3600 + day_data['frame_ts'].dt.minute * 60
    frame_counts = day_data.groupby('min_group').size().reset_index(name='frame_count')

    # Create a range of 10-minute intervals covering the a 24-hour period
    min_range = range(0, 24 * 60 * 60, 10 * 60)
    sns.histplot(data=frame_counts, x='min_group', bins=min_range, kde=True, ax=ax, alpha=0.5)


ax.set_xlabel('Frame Timestamp (Hour:Minute)')
ax.set_ylabel('Frequency')
ax.set_title('Distribution of Frame Timestamps for Each 24-Hour Period')
ax.set_xticklabels([pd.to_datetime(sec, unit='s').strftime('%H:%M') for sec in ax.get_xticks()])

plt.tight_layout()
plt.savefig('frame_distribution_plot1.png')
#plt.show()




data = pd.read_csv('df2.csv', usecols=['frame_ts'])

# Convert epoch timestamp to datetime
data['frame_ts'] = pd.to_datetime(data['frame_ts'], unit='ms')

# Group data into 10-minute intervals for the entire dataset
data['min_group'] = data['frame_ts'].dt.hour * 60 + data['frame_ts'].dt.minute
frame_counts = data.groupby('min_group').size().reset_index(name='frame_count')

# Create a range of 10-minute intervals covering the entire 24-hour period
min_range = range(0, 24 * 60, 30)  # Adjust the step size to 30 minutes

# Plot the distribution
plt.figure(figsize=(15, 8))
sns.histplot(data=frame_counts, x='min_group', bins=min_range, kde=True)
plt.xlabel('Frame Timestamp (Hour:Minute)')
plt.ylabel('Frequency')
plt.title('Distribution of Frame Timestamps for Each 24-Hour Period')
plt.xticks(min_range, [f'{hour:02d}:{minute:02d}' for hour in range(24) for minute in range(0, 60, 30)], rotation=45)
plt.tight_layout()
plt.savefig('frame_distribution_plot2.png')
#plt.show()








ef plot_frame_distribution_with_gaps(data, step_size=30):
    # Group data into 10-minute intervals for the entire dataset
    data['min_group'] = data['frame_ts'].dt.hour * 60 + data['frame_ts'].dt.minute
    frame_counts = data.groupby('min_group').size().reset_index(name='frame_count')

    # Create a range of 10-minute intervals covering the entire 24-hour period
    min_range = range(0, 24 * 60, step_size)

    # Plot the distribution
    plt.figure(figsize=(15, 8))
    sns.histplot(data=frame_counts, x='min_group', bins=min_range, kde=True)
    plt.xlabel('Frame Timestamp (Hour:Minute)')
    plt.ylabel('Frequency')
    plt.title('Distribution of Frame Timestamps for Each 24-Hour Period')
    plt.xticks(min_range, [f'{hour:02d}:{minute:02d}' for hour in range(24) for minute in range(0, 60, step_size)], rotation=45)

    # Find the gaps in data
    gap_indices = np.where(~frame_counts['min_group'].isin(min_range))[0]
    if len(gap_indices) > 0:
        for gap_index in gap_indices:
            plt.axvline(x=frame_counts.loc[gap_index, 'min_group'], color='red', linestyle='--', alpha=0.5)

    plt.tight_layout()
    plt.savefig('frame_distribution_plot_with_gaps.png')
    #plt.show()

# Example usage
data = pd.read_csv('df2.csv', usecols=['frame_ts'])
data['frame_ts'] = pd.to_datetime(data['frame_ts'], unit='ms')
plot_frame_distribution_with_gaps(data)


def find_missing_data(frame_counts):
    missing_data = []
    for i in range(0, 24 * 60, 30):
        if i not in frame_counts['min_group'].values:
            missing_data.append(f'{i // 60:02d}:{i % 60:02d}')
        elif frame_counts.loc[frame_counts['min_group'] == i, 'frame_count'].iloc[0] < 15:
            missing_data.append(f'{i // 60:02d}:{i % 60:02d}')
    return missing_data

missing_data = find_missing_data(frame_counts)
print("Times with no data or fewer than 15 frames:")
print(missing_data)
























import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
from adjustText import adjust_text

data = pd.read_csv('df2.csv', usecols=['video_id', 'frame_ts'])
# Convert epoch timestamp to datetime
data['frame_ts'] = pd.to_datetime(data['frame_ts'], unit='ms')

# Count the number of frames associated with each video ID
frame_counts = data.groupby('video_id').size()
data['frame_count'] = data['video_id'].map(frame_counts)

# Use pandas index to assign numeric IDs to each unique video ID
data['video_id_number'] = data.groupby('video_id').ngroup()

# Group data into 24-hour intervals
data['day'] = data['frame_ts'].dt.floor('D')

# Iterate over each day
for day, day_data in data.groupby('day'):
    # Check if the day has data within the 24-hour period
    start_period = day
    end_period = start_period + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)
    if not (start_period <= day_data['frame_ts'].max() <= end_period):
        continue

    # Group data into specified minute intervals for the current day
    mins = 10
    day_data['min_group'] = day_data['frame_ts'].dt.floor(f'{mins}min')

    # Get start and end timestamps for the distribution period
    start_period = start_period.strftime('%Y-%m-%d %H:%M:%S')
    end_period = end_period.strftime('%Y-%m-%d %H:%M:%S')

    # Plot
    plt.figure(figsize=(15, 8))
    ax = sns.histplot(data=day_data, x='min_group', hue='video_id_number', bins='auto', kde=False, palette='tab10')
    plt.xlabel(f'Frame Timestamp ({mins}-minute intervals)')
    plt.ylabel('Frequency')
    plt.title(f'Distribution of frame timestamps for each Video ID\nPeriod: {start_period} to {end_period}')

    # Set x-axis limits to cover the entire 24-hour period
    plt.xlim(day, day + pd.Timedelta(days=1))

    # Annotate bars with frame count
    texts = []

    for p in ax.patches:
        height = p.get_height()
        if height > 0:
            text = ax.annotate(f'{int(height)}', (p.get_x() + p.get_width() / 2., height), ha='center', va='bottom',
                               xytext=(0, 5), textcoords='offset points')
            texts.append(text)

    # Adjust the positions of the annotations to avoid overlaps
    #adjust_text(texts, expand_text=(1.01, 1.03))

    # Create custom legend with correct colors
    legend_handles = [Patch(color=color) for color in
                      sns.color_palette('tab10', n_colors=len(day_data['video_id_number'].unique()))]
    legend_labels = list(day_data['video_id_number'].unique())
    plt.legend(title='Video ID', labels=[str(label) for label in legend_labels], handles=legend_handles,
               loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=15)

    plt.tight_layout()
    filename = f'frame_distribution_plot_{day.strftime("%Y-%m-%d")}.png'
    plt.savefig(filename)
    #plt.show()
    plt.close()































import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('df2.csv', usecols=['video_id', 'frame_ts'])
#data = data[0:500000]

# Convert epoch timestamp to datetime
data['frame_ts'] = pd.to_datetime(data['frame_ts'], unit='ms')

video_id_numbers = {vid: i+1 for i, vid in enumerate(data['video_id'].unique())}

# Replace video_id with corresponding numbers
data['video_id_number'] = data['video_id'].map(video_id_numbers)

# Group data into specified minute intervals
mins = 10
data['min_group'] = data['frame_ts'].dt.floor(f'{mins}min')

# Plot
plt.figure(figsize=(15, 8))
ax = sns.histplot(data=data, x='min_group', hue='video_id_number', bins='auto', kde=False, palette='tab10')
plt.xlabel(f'Frame Timestamp ({mins}-minute intervals)')
plt.ylabel('Frequency')
plt.title('Distribution of frame timestamps for each Video ID')

legend_labels = [f'{num} ' for video_id, num in video_id_numbers.items()]

plt.legend(title='Video ID', labels=legend_labels, loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=15)
#plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
plt.savefig('frame_distribution_plot.png')
plt.close()


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Read the CSV file
data = pd.read_csv('df2.csv', usecols=['video_id', 'frame_ts'])
#data = data[0:500000]

data['frame_ts'] = pd.to_datetime(data['frame_ts'], unit='ms')

grouped_data = data.groupby('video_id')

mins = 5

for video_id, group_data in grouped_data:

    group_data['min_group'] = group_data['frame_ts'].dt.floor(f'{mins}min')

    # Plot histogram for the current video
    plt.figure(figsize=(8, 6))
    sns.histplot(data=group_data, x='min_group', bins=15, kde=True)
    plt.xlabel(f'Frame TS ({mins}-minute intervals)')
    plt.ylabel('Frequency')
    plt.title(f'{video_id}')

    plt.savefig(f'images/{video_id}_frame_distribution_plot.png')
    plt.close()

































import asyncio
import aioboto3
from boto3.dynamodb.conditions import Key

async def query_video(table, video_id, exclusive_start_key=None):
    if exclusive_start_key:
        response = await table.query(
            KeyConditionExpression=Key('video_id').eq(video_id),
            ExclusiveStartKey=exclusive_start_key
        )
    else:
        response = await table.query(
            KeyConditionExpression=Key('video_id').eq(video_id)
        )
    return response['Items'], response.get('LastEvaluatedKey')

async def process_batch(table, video_ids):
    items = []
    async with aioboto3.resource('dynamodb') as dynamodb:
        for video_id in video_ids:
            last_evaluated_key = None
            while True:
                result, last_evaluated_key = await query_video(dynamodb.Table(table), video_id, last_evaluated_key)
                items.extend(result)
                if not last_evaluated_key:
                    break
    return items

async def main():
    # Convert set to list
    matching_ids_list = list(matching_ids_list)

    # Chunk the matching_ids_list into smaller batches
    batch_size = 10
    batches = [matching_ids_list[i:i + batch_size] for i in range(0, len(matching_ids_list), batch_size)]

    # Process batches in parallel
    items = []
    async with aioboto3.resource('dynamodb') as dynamodb:
        tasks = [process_batch(dynamodb.Table(dy_table_name), batch) for batch in batches]
        results = await asyncio.gather(*tasks)
        for result in results:
            items.extend(result)
    
    return items

# Run the asyncio event loop
if __name__ == "__main__":
    items = asyncio.run(main())







import re

pattern = re.compile(r'(\d{4}-\d{2}-\d{2}_\d{2}-\d{2}-\d{2})')


def extract_info(file_name):
    match = pattern.search(file_name)
    if match:
        group_name = match.group(1)
        datetime_str = match.group(2)
        return group_name, datetime_str
    else:
        return None, None

# Example usage
file_name = "(10.114.237.108) - TV401C PC1 ROM Bin-2024.04.12-15.00.00-15m00s_0011267.jpg"
group_name, datetime_str = extract_info(file_name)
print("Group Name:", group_name)
print("Datetime:", datetime_str)

import pandas as pd
import re
import matplotlib.pyplot as plt

def plot_file_date_distribution(file_list):
    # Create an empty DataFrame to store parsed data
    df = pd.DataFrame(columns=['File_Name', 'Group_Name', 'Datetime'])
    
    # Regular expression pattern to extract datetime and group name
    pattern = r'(TV.+ Bin)[-_](\d{4}\.\d{2}\.\d{2}-\d{2}\.\d{2}\.\d{2})_'
    
    # Iterate through each file name
    for file_name in file_list:
        print(file_name)
        # Extract group name and datetime using regex
        match = re.search(pattern, file_name)
        print(match)
        if match:
            group_name = match.group(1)
            datetime_str = match.group(2)
            print(group_name)
            # Append data to DataFrame
            df = df.append({'File_Name': file_name, 'Group_Name': group_name, 'Datetime': datetime_str}, ignore_index=True)
            
    # Convert 'Datetime' column to datetime format
    df['Datetime'] = pd.to_datetime(df['Datetime'], format='%Y-%m-%d_%H-%M-%S')
     
    # Bin the datetimes into groups of 10 minutes
    df['Binned_Datetime'] = df['Datetime'].dt.floor('10min')
 
    # Plot distribution for each unique group
    unique_groups = df['Group_Name'].unique()
   
    for group_name in unique_groups:
        group_df = df[df['Group_Name'] == group_name]
        plt.hist(group_df['Binned_Datetime'], bins=pd.date_range(start=group_df['Binned_Datetime'].min(), end=group_df['Binned_Datetime'].max(), freq='10min'), alpha=0.5, label=group_name)

    plt.xlabel('Time')
    plt.ylabel('Frequency')
    plt.title('File Date Distribution')
    #plt.legend()
    plt.show()
 
plot_file_date_distribution(images)



(10.114.237.108) - TV401C PC1 ROM Bin-2024.04.12-15.00.00-15m00s_0000012.jpg
TV404C PC2 ROM Bin North West_urn-uuid-00075fbe-4138-3841-be5f-0700075fbe5f_2024-05-03_05-00-00(1)_0003770.jpg





RuntimeError                              Traceback (most recent call last)
Cell In[86], line 2
      1 loop = asyncio.get_event_loop()
----> 2 items = loop.run_until_complete(main())

File /usr/local/lib/python3.10/asyncio/base_events.py:625, in BaseEventLoop.run_until_complete(self, future)
    614 """Run until the Future is done.
    615 
    616 If the argument is a coroutine, it is wrapped in a Task.
   (...)
    622 Return the Future's result, or raise its exception.
    623 """
    624 self._check_closed()
--> 625 self._check_running()
    627 new_task = not futures.isfuture(future)
    628 future = tasks.ensure_future(future, loop=self)

File /usr/local/lib/python3.10/asyncio/base_events.py:584, in BaseEventLoop._check_running(self)
    582 def _check_running(self):
    583     if self.is_running():
--> 584         raise RuntimeError('This event loop is already running')
    585     if events._get_running_loop() is not None:
    586         raise RuntimeError(
    587             'Cannot run the event loop while another loop is running')

RuntimeError: This event loop is already running








import boto3
from urllib.parse import urlparse

# Assuming you have already initialized your S3 client
s3 = boto3.client('s3')

uri = 's3://dev-iroccvdmsstack-dmsbucketb119a736-rtlxxvez6thx/frames/TV401C PC1 ROM Bin_urn-uuid-00075fbe-43fb-fb43-be5f-0700075fbe5f_2024-05-02_08-00-00(3)/'
parsed_uri = urlparse(uri)

prefix_frames22 = []

# Extract bucket name and object key prefix from the parsed URI
bucket_name = parsed_uri.netloc
prefix = parsed_uri.path.lstrip('/')

# Paginate through the results to retrieve all objects
paginator = s3.get_paginator('list_objects_v2')
pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

for page in pages:
    object_keys = [obj['Key'] for obj in page.get('Contents', [])]
    prefix_frames22.extend(object_keys)

print(len(prefix_frames22))
