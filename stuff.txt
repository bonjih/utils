iimport asyncio
import aioboto3
from boto3.dynamodb.conditions import Key

async def query_video(table, video_id, exclusive_start_key=None):
    if exclusive_start_key:
        response = await table.query(
            KeyConditionExpression=Key('video_id').eq(video_id),
            ExclusiveStartKey=exclusive_start_key
        )
    else:
        response = await table.query(
            KeyConditionExpression=Key('video_id').eq(video_id)
        )
    return response['Items'], response.get('LastEvaluatedKey')

async def process_batch(table, video_ids):
    items = []
    async with aioboto3.resource('dynamodb') as dynamodb:
        for video_id in video_ids:
            last_evaluated_key = None
            while True:
                result, last_evaluated_key = await query_video(dynamodb.Table(table), video_id, last_evaluated_key)
                items.extend(result)
                if not last_evaluated_key:
                    break
    return items

async def main():
    # Convert set to list
    matching_ids_list = list(matching_ids_list)

    # Chunk the matching_ids_list into smaller batches
    batch_size = 10
    batches = [matching_ids_list[i:i + batch_size] for i in range(0, len(matching_ids_list), batch_size)]

    # Process batches in parallel
    items = []
    async with aioboto3.resource('dynamodb') as dynamodb:
        tasks = [process_batch(dynamodb.Table(dy_table_name), batch) for batch in batches]
        results = await asyncio.gather(*tasks)
        for result in results:
            items.extend(result)
    
    return items

# Run the asyncio event loop
if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    items = loop.run_until_complete(main())










import re

pattern = re.compile(r'(\d{4}-\d{2}-\d{2}_\d{2}-\d{2}-\d{2})')


def extract_info(file_name):
    match = pattern.search(file_name)
    if match:
        group_name = match.group(1)
        datetime_str = match.group(2)
        return group_name, datetime_str
    else:
        return None, None

# Example usage
file_name = "(10.114.237.108) - TV401C PC1 ROM Bin-2024.04.12-15.00.00-15m00s_0011267.jpg"
group_name, datetime_str = extract_info(file_name)
print("Group Name:", group_name)
print("Datetime:", datetime_str)

import pandas as pd
import re
import matplotlib.pyplot as plt

def plot_file_date_distribution(file_list):
    # Create an empty DataFrame to store parsed data
    df = pd.DataFrame(columns=['File_Name', 'Group_Name', 'Datetime'])
    
    # Regular expression pattern to extract datetime and group name
    pattern = r'(TV.+ Bin)[-_](\d{4}\.\d{2}\.\d{2}-\d{2}\.\d{2}\.\d{2})_'
    
    # Iterate through each file name
    for file_name in file_list:
        print(file_name)
        # Extract group name and datetime using regex
        match = re.search(pattern, file_name)
        print(match)
        if match:
            group_name = match.group(1)
            datetime_str = match.group(2)
            print(group_name)
            # Append data to DataFrame
            df = df.append({'File_Name': file_name, 'Group_Name': group_name, 'Datetime': datetime_str}, ignore_index=True)
            
    # Convert 'Datetime' column to datetime format
    df['Datetime'] = pd.to_datetime(df['Datetime'], format='%Y-%m-%d_%H-%M-%S')
     
    # Bin the datetimes into groups of 10 minutes
    df['Binned_Datetime'] = df['Datetime'].dt.floor('10min')
 
    # Plot distribution for each unique group
    unique_groups = df['Group_Name'].unique()
   
    for group_name in unique_groups:
        group_df = df[df['Group_Name'] == group_name]
        plt.hist(group_df['Binned_Datetime'], bins=pd.date_range(start=group_df['Binned_Datetime'].min(), end=group_df['Binned_Datetime'].max(), freq='10min'), alpha=0.5, label=group_name)

    plt.xlabel('Time')
    plt.ylabel('Frequency')
    plt.title('File Date Distribution')
    #plt.legend()
    plt.show()
 
plot_file_date_distribution(images)



(10.114.237.108) - TV401C PC1 ROM Bin-2024.04.12-15.00.00-15m00s_0000012.jpg
TV404C PC2 ROM Bin North West_urn-uuid-00075fbe-4138-3841-be5f-0700075fbe5f_2024-05-03_05-00-00(1)_0003770.jpg





RuntimeError                              Traceback (most recent call last)
Cell In[86], line 2
      1 loop = asyncio.get_event_loop()
----> 2 items = loop.run_until_complete(main())

File /usr/local/lib/python3.10/asyncio/base_events.py:625, in BaseEventLoop.run_until_complete(self, future)
    614 """Run until the Future is done.
    615 
    616 If the argument is a coroutine, it is wrapped in a Task.
   (...)
    622 Return the Future's result, or raise its exception.
    623 """
    624 self._check_closed()
--> 625 self._check_running()
    627 new_task = not futures.isfuture(future)
    628 future = tasks.ensure_future(future, loop=self)

File /usr/local/lib/python3.10/asyncio/base_events.py:584, in BaseEventLoop._check_running(self)
    582 def _check_running(self):
    583     if self.is_running():
--> 584         raise RuntimeError('This event loop is already running')
    585     if events._get_running_loop() is not None:
    586         raise RuntimeError(
    587             'Cannot run the event loop while another loop is running')

RuntimeError: This event loop is already running








import boto3
from urllib.parse import urlparse

# Assuming you have already initialized your S3 client
s3 = boto3.client('s3')

uri = 's3://dev-iroccvdmsstack-dmsbucketb119a736-rtlxxvez6thx/frames/TV401C PC1 ROM Bin_urn-uuid-00075fbe-43fb-fb43-be5f-0700075fbe5f_2024-05-02_08-00-00(3)/'
parsed_uri = urlparse(uri)

prefix_frames22 = []

# Extract bucket name and object key prefix from the parsed URI
bucket_name = parsed_uri.netloc
prefix = parsed_uri.path.lstrip('/')

# Paginate through the results to retrieve all objects
paginator = s3.get_paginator('list_objects_v2')
pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

for page in pages:
    object_keys = [obj['Key'] for obj in page.get('Contents', [])]
    prefix_frames22.extend(object_keys)

print(len(prefix_frames22))
