table = dynamodb.Table(dy_table_name)

items = []

for video in matching_ids_list:
    response = table.query(
        KeyConditionExpression=Key('video_id').eq(video),
    )

    items.extend(response['Items'])

    # Include this for large queries
    while 'LastEvaluatedKey' in response:
        response = table.query(
            KeyConditionExpression=Key('video_id').eq(video),
            ExclusiveStartKey=response['LastEvaluatedKey']
        )

        items.extend(response['Items'])










import re

pattern = re.compile(r'(\d{4}-\d{2}-\d{2}_\d{2}-\d{2}-\d{2})')


def extract_info(file_name):
    match = pattern.search(file_name)
    if match:
        group_name = match.group(1)
        datetime_str = match.group(2)
        return group_name, datetime_str
    else:
        return None, None

# Example usage
file_name = "(10.114.237.108) - TV401C PC1 ROM Bin-2024.04.12-15.00.00-15m00s_0011267.jpg"
group_name, datetime_str = extract_info(file_name)
print("Group Name:", group_name)
print("Datetime:", datetime_str)

import pandas as pd
import re
import matplotlib.pyplot as plt

def plot_file_date_distribution(file_list):
    # Create an empty DataFrame to store parsed data
    df = pd.DataFrame(columns=['File_Name', 'Group_Name', 'Datetime'])
    
    # Regular expression pattern to extract datetime and group name
    pattern = r'(TV.+ Bin)[-_](\d{4}\.\d{2}\.\d{2}-\d{2}\.\d{2}\.\d{2})_'
    
    # Iterate through each file name
    for file_name in file_list:
        print(file_name)
        # Extract group name and datetime using regex
        match = re.search(pattern, file_name)
        print(match)
        if match:
            group_name = match.group(1)
            datetime_str = match.group(2)
            print(group_name)
            # Append data to DataFrame
            df = df.append({'File_Name': file_name, 'Group_Name': group_name, 'Datetime': datetime_str}, ignore_index=True)
            
    # Convert 'Datetime' column to datetime format
    df['Datetime'] = pd.to_datetime(df['Datetime'], format='%Y-%m-%d_%H-%M-%S')
     
    # Bin the datetimes into groups of 10 minutes
    df['Binned_Datetime'] = df['Datetime'].dt.floor('10min')
 
    # Plot distribution for each unique group
    unique_groups = df['Group_Name'].unique()
   
    for group_name in unique_groups:
        group_df = df[df['Group_Name'] == group_name]
        plt.hist(group_df['Binned_Datetime'], bins=pd.date_range(start=group_df['Binned_Datetime'].min(), end=group_df['Binned_Datetime'].max(), freq='10min'), alpha=0.5, label=group_name)

    plt.xlabel('Time')
    plt.ylabel('Frequency')
    plt.title('File Date Distribution')
    #plt.legend()
    plt.show()
 
plot_file_date_distribution(images)



(10.114.237.108) - TV401C PC1 ROM Bin-2024.04.12-15.00.00-15m00s_0000012.jpg
TV404C PC2 ROM Bin North West_urn-uuid-00075fbe-4138-3841-be5f-0700075fbe5f_2024-05-03_05-00-00(1)_0003770.jpg














import boto3
from urllib.parse import urlparse

# Assuming you have already initialized your S3 client
s3 = boto3.client('s3')

uri = 's3://dev-iroccvdmsstack-dmsbucketb119a736-rtlxxvez6thx/frames/TV401C PC1 ROM Bin_urn-uuid-00075fbe-43fb-fb43-be5f-0700075fbe5f_2024-05-02_08-00-00(3)/'
parsed_uri = urlparse(uri)

prefix_frames22 = []

# Extract bucket name and object key prefix from the parsed URI
bucket_name = parsed_uri.netloc
prefix = parsed_uri.path.lstrip('/')

# Paginate through the results to retrieve all objects
paginator = s3.get_paginator('list_objects_v2')
pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

for page in pages:
    object_keys = [obj['Key'] for obj in page.get('Contents', [])]
    prefix_frames22.extend(object_keys)

print(len(prefix_frames22))
