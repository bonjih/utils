table = dynamodb.Table(dy_table_name)

items = []

for video in matching_ids_list:
    response = table.query(
        KeyConditionExpression=Key('video_id').eq(video),
    )

    items.extend(response['Items'])

    # Include this for large queries
    while 'LastEvaluatedKey' in response:
        response = table.query(
            KeyConditionExpression=Key('video_id').eq(video),
            ExclusiveStartKey=response['LastEvaluatedKey']
        )

        items.extend(response['Items'])










data['min_group'] = data['frame_ts'].dt.hour * 60 + data['frame_ts'].dt.minute
frame_counts = data.groupby('min_group').size().reset_index(name='frame_count')

# Create a range of 10-minute intervals covering the entire 24-hour period
min_range = range(0, 24 * 60, 30)  # Adjust the step size to 30 minutes

# Plot the distribution
plt.figure(figsize=(15, 8))
sns.histplot(data=frame_counts, x='min_group', bins=min_range, kde=True)
plt.xlabel('Frame Timestamp (Hour:Minute)')
plt.ylabel('Frequency')
plt.title('Distribution of Frame Timestamps for Each 24-Hour Period')

# Add markers for bridged events
bridged_data = data[data['bridged'] == True]
plt.scatter(bridged_data['min_group'], [0]*len(bridged_data), marker='_',   color='red', label='Bridged Events')

plt.xticks(min_range, [f'{hour:02d}:{minute:02d}' for hour in range(24) for minute in range(0, 60, 30)], rotation=45)
plt.tight_layout()
plt.legend()
plt.savefig('frame_distribution_with_bridged_events.png')
plt.show()


def get_matching_frames(id_list):
    # for some reason, need to construct uri, unpick and parse
    prefix_frames = []
    
    for i in id_list:
        uri = f's3://{bucket_dms}/{frames_dms}{i}/'
        parsed_uri = urlparse(uri)

        # extract bucket name and object key prefix from the parsed URI
        bucket_name = parsed_uri.netloc
        prefix = parsed_uri.path.lstrip('/')
        
        # paginate through the results to retrieve all objects
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

        for page in pages:
            object_keys = [obj['Key'] for obj in page.get('Contents', [])]
            prefix_frames.extend(object_keys)

    return prefix_frames

prefix_list = get_matching_frames(list(matching_ids_list))


import concurrent.futures
import boto3
from urllib.parse import urlparse

# Initialize S3 client
s3 = boto3.client('s3')

def get_matching_frames(id_list):
    prefix_frames = []

    def fetch_prefix_frames(uri):
        parsed_uri = urlparse(uri)
        bucket_name = parsed_uri.netloc
        prefix = parsed_uri.path.lstrip('/')

        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

        object_keys = []
        for page in pages:
            object_keys.extend([obj['Key'] for obj in page.get('Contents', [])])

        return object_keys

    # Use ThreadPoolExecutor to run fetch_prefix_frames asynchronously
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_to_uri = {executor.submit(fetch_prefix_frames, f's3://{bucket_dms}/{frames_dms}{i}/'): i for i in id_list}
        for future in concurrent.futures.as_completed(future_to_uri):
            uri = future_to_uri[future]
            try:
                object_keys = future.result()
                prefix_frames.extend(object_keys)
            except Exception as exc:
                print(f'Error fetching frames for {uri}: {exc}')

    return prefix_frames

prefix_list = get_matching_frames(matching_ids_list)











def get_matching_frames(id_list):
    # for some reason, need to construct uri, unpick and parse
    prefix_frames = []
    
    for i in id_list:
        uri = f's3://{bucket_dms}/{frames_dms}{i}/'
        parsed_uri = urlparse(uri)

        # extract bucket name and object key prefix from the parsed URI
        bucket_name = parsed_uri.netloc
        prefix = parsed_uri.path.lstrip('/')
        
        # paginate through the results to retrieve all objects
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

        for page in pages:
            object_keys = [obj['Key'] for obj in page.get('Contents', [])]
            prefix_frames.extend(object_keys)

    return prefix_frames

prefix_list = get_matching_frames(list(matching_ids_list))
print(len(prefix_list))





















import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv('df2.csv', usecols=['frame_ts'])
data['frame_ts'] = pd.to_datetime(data['frame_ts'], unit='ms')

grouped_data = data.groupby(data['frame_ts'].dt.floor('D'))

fig, ax = plt.subplots(figsize=(15, 8))

# Iterate over each day and plot the distribution
for day, day_data in grouped_data:
    # Group data into 10-minute intervals within the current day and count the number of frames
    day_data['min_group'] = day_data['frame_ts'].dt.hour * 3600 + day_data['frame_ts'].dt.minute * 60
    frame_counts = day_data.groupby('min_group').size().reset_index(name='frame_count')

    # Create a range of 10-minute intervals covering the a 24-hour period
    min_range = range(0, 24 * 60 * 60, 10 * 60)
    sns.histplot(data=frame_counts, x='min_group', bins=min_range, kde=True, ax=ax, alpha=0.5)


ax.set_xlabel('Frame Timestamp (Hour:Minute)')
ax.set_ylabel('Frequency')
ax.set_title('Distribution of Frame Timestamps for Each 24-Hour Period')
ax.set_xticklabels([pd.to_datetime(sec, unit='s').strftime('%H:%M') for sec in ax.get_xticks()])

plt.tight_layout()
plt.savefig('frame_distribution_plot1.png')
#plt.show()




data = pd.read_csv('df2.csv', usecols=['frame_ts'])

# Convert epoch timestamp to datetime
data['frame_ts'] = pd.to_datetime(data['frame_ts'], unit='ms')

# Group data into 10-minute intervals for the entire dataset
data['min_group'] = data['frame_ts'].dt.hour * 60 + data['frame_ts'].dt.minute
frame_counts = data.groupby('min_group').size().reset_index(name='frame_count')

# Create a range of 10-minute intervals covering the entire 24-hour period
min_range = range(0, 24 * 60, 30)  # Adjust the step size to 30 minutes

# Plot the distribution
plt.figure(figsize=(15, 8))
sns.histplot(data=frame_counts, x='min_group', bins=min_range, kde=True)
plt.xlabel('Frame Timestamp (Hour:Minute)')
plt.ylabel('Frequency')
plt.title('Distribution of Frame Timestamps for Each 24-Hour Period')
plt.xticks(min_range, [f'{hour:02d}:{minute:02d}' for hour in range(24) for minute in range(0, 60, 30)], rotation=45)
plt.tight_layout()
plt.savefig('frame_distribution_plot2.png')
#plt.show()








ef plot_frame_distribution_with_gaps(data, step_size=30):
    # Group data into 10-minute intervals for the entire dataset
    data['min_group'] = data['frame_ts'].dt.hour * 60 + data['frame_ts'].dt.minute
    frame_counts = data.groupby('min_group').size().reset_index(name='frame_count')

    # Create a range of 10-minute intervals covering the entire 24-hour period
    min_range = range(0, 24 * 60, step_size)

    # Plot the distribution
    plt.figure(figsize=(15, 8))
    sns.histplot(data=frame_counts, x='min_group', bins=min_range, kde=True)
    plt.xlabel('Frame Timestamp (Hour:Minute)')
    plt.ylabel('Frequency')
    plt.title('Distribution of Frame Timestamps for Each 24-Hour Period')
    plt.xticks(min_range, [f'{hour:02d}:{minute:02d}' for hour in range(24) for minute in range(0, 60, step_size)], rotation=45)

    # Find the gaps in data
    gap_indices = np.where(~frame_counts['min_group'].isin(min_range))[0]
    if len(gap_indices) > 0:
        for gap_index in gap_indices:
            plt.axvline(x=frame_counts.loc[gap_index, 'min_group'], color='red', linestyle='--', alpha=0.5)

    plt.tight_layout()
    plt.savefig('frame_distribution_plot_with_gaps.png')
    #plt.show()

# Example usage
data = pd.read_csv('df2.csv', usecols=['frame_ts'])
data['frame_ts'] = pd.to_datetime(data['frame_ts'], unit='ms')
plot_frame_distribution_with_gaps(data)


def find_missing_data(frame_counts):
    missing_data = []
    for i in range(0, 24 * 60, 30):
        if i not in frame_counts['min_group'].values:
            missing_data.append(f'{i // 60:02d}:{i % 60:02d}')
        elif frame_counts.loc[frame_counts['min_group'] == i, 'frame_count'].iloc[0] < 15:
            missing_data.append(f'{i // 60:02d}:{i % 60:02d}')
    return missing_data

missing_data = find_missing_data(frame_counts)
print("Times with no data or fewer than 15 frames:")
print(missing_data)
























import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
from adjustText import adjust_text

data = pd.read_csv('df2.csv', usecols=['video_id', 'frame_ts'])
# Convert epoch timestamp to datetime
data['frame_ts'] = pd.to_datetime(data['frame_ts'], unit='ms')

# Count the number of frames associated with each video ID
frame_counts = data.groupby('video_id').size()
data['frame_count'] = data['video_id'].map(frame_counts)

# Use pandas index to assign numeric IDs to each unique video ID
data['video_id_number'] = data.groupby('video_id').ngroup()

# Group data into 24-hour intervals
data['day'] = data['frame_ts'].dt.floor('D')

# Iterate over each day
for day, day_data in data.groupby('day'):
    # Check if the day has data within the 24-hour period
    start_period = day
    end_period = start_period + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)
    if not (start_period <= day_data['frame_ts'].max() <= end_period):
        continue

    # Group data into specified minute intervals for the current day
    mins = 10
    day_data['min_group'] = day_data['frame_ts'].dt.floor(f'{mins}min')

    # Get start and end timestamps for the distribution period
    start_period = start_period.strftime('%Y-%m-%d %H:%M:%S')
    end_period = end_period.strftime('%Y-%m-%d %H:%M:%S')

    # Plot
    plt.figure(figsize=(15, 8))
    ax = sns.histplot(data=day_data, x='min_group', hue='video_id_number', bins='auto', kde=False, palette='tab10')
    plt.xlabel(f'Frame Timestamp ({mins}-minute intervals)')
    plt.ylabel('Frequency')
    plt.title(f'Distribution of frame timestamps for each Video ID\nPeriod: {start_period} to {end_period}')

    # Set x-axis limits to cover the entire 24-hour period
    plt.xlim(day, day + pd.Timedelta(days=1))

    # Annotate bars with frame count
    texts = []

    for p in ax.patches:
        height = p.get_height()
        if height > 0:
            text = ax.annotate(f'{int(height)}', (p.get_x() + p.get_width() / 2., height), ha='center', va='bottom',
                               xytext=(0, 5), textcoords='offset points')
            texts.append(text)

    # Adjust the positions of the annotations to avoid overlaps
    #adjust_text(texts, expand_text=(1.01, 1.03))

    # Create custom legend with correct colors
    legend_handles = [Patch(color=color) for color in
                      sns.color_palette('tab10', n_colors=len(day_data['video_id_number'].unique()))]
    legend_labels = list(day_data['video_id_number'].unique())
    plt.legend(title='Video ID', labels=[str(label) for label in legend_labels], handles=legend_handles,
               loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=15)

    plt.tight_layout()
    filename = f'frame_distribution_plot_{day.strftime("%Y-%m-%d")}.png'
    plt.savefig(filename)
    #plt.show()
    plt.close()































import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('df2.csv', usecols=['video_id', 'frame_ts'])
#data = data[0:500000]

# Convert epoch timestamp to datetime
data['frame_ts'] = pd.to_datetime(data['frame_ts'], unit='ms')

video_id_numbers = {vid: i+1 for i, vid in enumerate(data['video_id'].unique())}

# Replace video_id with corresponding numbers
data['video_id_number'] = data['video_id'].map(video_id_numbers)

# Group data into specified minute intervals
mins = 10
data['min_group'] = data['frame_ts'].dt.floor(f'{mins}min')

# Plot
plt.figure(figsize=(15, 8))
ax = sns.histplot(data=data, x='min_group', hue='video_id_number', bins='auto', kde=False, palette='tab10')
plt.xlabel(f'Frame Timestamp ({mins}-minute intervals)')
plt.ylabel('Frequency')
plt.title('Distribution of frame timestamps for each Video ID')

legend_labels = [f'{num} ' for video_id, num in video_id_numbers.items()]

plt.legend(title='Video ID', labels=legend_labels, loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=15)
#plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
plt.savefig('frame_distribution_plot.png')
plt.close()


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Read the CSV file
data = pd.read_csv('df2.csv', usecols=['video_id', 'frame_ts'])
#data = data[0:500000]

data['frame_ts'] = pd.to_datetime(data['frame_ts'], unit='ms')

grouped_data = data.groupby('video_id')

mins = 5

for video_id, group_data in grouped_data:

    group_data['min_group'] = group_data['frame_ts'].dt.floor(f'{mins}min')

    # Plot histogram for the current video
    plt.figure(figsize=(8, 6))
    sns.histplot(data=group_data, x='min_group', bins=15, kde=True)
    plt.xlabel(f'Frame TS ({mins}-minute intervals)')
    plt.ylabel('Frequency')
    plt.title(f'{video_id}')

    plt.savefig(f'images/{video_id}_frame_distribution_plot.png')
    plt.close()

































import asyncio
import aioboto3
from boto3.dynamodb.conditions import Key

async def query_video(table, video_id, exclusive_start_key=None):
    if exclusive_start_key:
        response = await table.query(
            KeyConditionExpression=Key('video_id').eq(video_id),
            ExclusiveStartKey=exclusive_start_key
        )
    else:
        response = await table.query(
            KeyConditionExpression=Key('video_id').eq(video_id)
        )
    return response['Items'], response.get('LastEvaluatedKey')

async def process_batch(table, video_ids):
    items = []
    async with aioboto3.resource('dynamodb') as dynamodb:
        for video_id in video_ids:
            last_evaluated_key = None
            while True:
                result, last_evaluated_key = await query_video(dynamodb.Table(table), video_id, last_evaluated_key)
                items.extend(result)
                if not last_evaluated_key:
                    break
    return items

async def main():
    # Convert set to list
    matching_ids_list = list(matching_ids_list)

    # Chunk the matching_ids_list into smaller batches
    batch_size = 10
    batches = [matching_ids_list[i:i + batch_size] for i in range(0, len(matching_ids_list), batch_size)]

    # Process batches in parallel
    items = []
    async with aioboto3.resource('dynamodb') as dynamodb:
        tasks = [process_batch(dynamodb.Table(dy_table_name), batch) for batch in batches]
        results = await asyncio.gather(*tasks)
        for result in results:
            items.extend(result)
    
    return items

# Run the asyncio event loop
if __name__ == "__main__":
    items = asyncio.run(main())







import re

pattern = re.compile(r'(\d{4}-\d{2}-\d{2}_\d{2}-\d{2}-\d{2})')


def extract_info(file_name):
    match = pattern.search(file_name)
    if match:
        group_name = match.group(1)
        datetime_str = match.group(2)
        return group_name, datetime_str
    else:
        return None, None

# Example usage
file_name = "(10.114.237.108) - TV401C PC1 ROM Bin-2024.04.12-15.00.00-15m00s_0011267.jpg"
group_name, datetime_str = extract_info(file_name)
print("Group Name:", group_name)
print("Datetime:", datetime_str)

import pandas as pd
import re
import matplotlib.pyplot as plt

def plot_file_date_distribution(file_list):
    # Create an empty DataFrame to store parsed data
    df = pd.DataFrame(columns=['File_Name', 'Group_Name', 'Datetime'])
    
    # Regular expression pattern to extract datetime and group name
    pattern = r'(TV.+ Bin)[-_](\d{4}\.\d{2}\.\d{2}-\d{2}\.\d{2}\.\d{2})_'
    
    # Iterate through each file name
    for file_name in file_list:
        print(file_name)
        # Extract group name and datetime using regex
        match = re.search(pattern, file_name)
        print(match)
        if match:
            group_name = match.group(1)
            datetime_str = match.group(2)
            print(group_name)
            # Append data to DataFrame
            df = df.append({'File_Name': file_name, 'Group_Name': group_name, 'Datetime': datetime_str}, ignore_index=True)
            
    # Convert 'Datetime' column to datetime format
    df['Datetime'] = pd.to_datetime(df['Datetime'], format='%Y-%m-%d_%H-%M-%S')
     
    # Bin the datetimes into groups of 10 minutes
    df['Binned_Datetime'] = df['Datetime'].dt.floor('10min')
 
    # Plot distribution for each unique group
    unique_groups = df['Group_Name'].unique()
   
    for group_name in unique_groups:
        group_df = df[df['Group_Name'] == group_name]
        plt.hist(group_df['Binned_Datetime'], bins=pd.date_range(start=group_df['Binned_Datetime'].min(), end=group_df['Binned_Datetime'].max(), freq='10min'), alpha=0.5, label=group_name)

    plt.xlabel('Time')
    plt.ylabel('Frequency')
    plt.title('File Date Distribution')
    #plt.legend()
    plt.show()
 
plot_file_date_distribution(images)



(10.114.237.108) - TV401C PC1 ROM Bin-2024.04.12-15.00.00-15m00s_0000012.jpg
TV404C PC2 ROM Bin North West_urn-uuid-00075fbe-4138-3841-be5f-0700075fbe5f_2024-05-03_05-00-00(1)_0003770.jpg





RuntimeError                              Traceback (most recent call last)
Cell In[86], line 2
      1 loop = asyncio.get_event_loop()
----> 2 items = loop.run_until_complete(main())

File /usr/local/lib/python3.10/asyncio/base_events.py:625, in BaseEventLoop.run_until_complete(self, future)
    614 """Run until the Future is done.
    615 
    616 If the argument is a coroutine, it is wrapped in a Task.
   (...)
    622 Return the Future's result, or raise its exception.
    623 """
    624 self._check_closed()
--> 625 self._check_running()
    627 new_task = not futures.isfuture(future)
    628 future = tasks.ensure_future(future, loop=self)

File /usr/local/lib/python3.10/asyncio/base_events.py:584, in BaseEventLoop._check_running(self)
    582 def _check_running(self):
    583     if self.is_running():
--> 584         raise RuntimeError('This event loop is already running')
    585     if events._get_running_loop() is not None:
    586         raise RuntimeError(
    587             'Cannot run the event loop while another loop is running')

RuntimeError: This event loop is already running








import boto3
from urllib.parse import urlparse

# Assuming you have already initialized your S3 client
s3 = boto3.client('s3')

uri = 's3://dev-iroccvdmsstack-dmsbucketb119a736-rtlxxvez6thx/frames/TV401C PC1 ROM Bin_urn-uuid-00075fbe-43fb-fb43-be5f-0700075fbe5f_2024-05-02_08-00-00(3)/'
parsed_uri = urlparse(uri)

prefix_frames22 = []

# Extract bucket name and object key prefix from the parsed URI
bucket_name = parsed_uri.netloc
prefix = parsed_uri.path.lstrip('/')

# Paginate through the results to retrieve all objects
paginator = s3.get_paginator('list_objects_v2')
pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

for page in pages:
    object_keys = [obj['Key'] for obj in page.get('Contents', [])]
    prefix_frames22.extend(object_keys)

print(len(prefix_frames22))
